{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd5c2fbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(442, 10)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "data = load_diabetes()\n",
    "X = data.data\n",
    "y = data.target\n",
    "X.shape\n",
    "\n",
    "# Linear regression is a statistical method used to model the relationship between a dependent variable (often\n",
    "# denoted as yyy) and one or more independent variables (denoted as XXX). The goal is to find the linear\n",
    "# relationship that best predicts yyy from XXX."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f42f427",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  6.50828076   1.28806748  20.8265647   15.61326631   7.24508406\n",
      "   5.85554099 -13.90078374  15.01590614  19.99313426  13.39901343]\n"
     ]
    }
   ],
   "source": [
    "# Gradient descent\n",
    "# Gradient descent is an iterative optimization algorithm used to minimize the cost function in linear regression. The\n",
    "# goal is to find the coefficients (weights) of the linear model that minimize the difference between the predicted\n",
    "# and actual values of the dependent variable.\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "def gradient_descent(X, y, alpha=0.01, iterations=1000):\n",
    "    m, n = X.shape\n",
    "    theta = np.zeros(n)\n",
    "    cost_history = []\n",
    "    for _ in range(iterations):\n",
    "        predictions = X.dot(theta)\n",
    "        errors = predictions - y\n",
    "        gradient = (1/m) * X.T.dot(errors)\n",
    "        theta -= alpha * gradient\n",
    "        cost = (1/(2*m)) * np.sum(errors**2)\n",
    "        cost_history.append(cost)\n",
    "    return theta, cost_history\n",
    "theta,cost_history=gradient_descent(X,y)\n",
    "print(theta)\n",
    "\n",
    "\n",
    "# Gradient descent can efficiently find the optimal solution but may converge to a local minimum if the cost\n",
    "# function is non-convex. The learning rate α\\alphaα plays a crucial role in the convergence speed and stability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0a1cd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ -10.0098663  -239.81564367  519.84592005  324.3846455  -792.17563855\n",
      "  476.73902101  101.04326794  177.06323767  751.27369956   67.62669218]\n"
     ]
    }
   ],
   "source": [
    "# LEAST SQUARES LINEAR REGRESSION\n",
    "\n",
    "# Least squares is a traditional approach to finding the coefficients that minimize the sum of squared differences\n",
    "# between observed and predicted values.\n",
    "\n",
    "def least_squares(X, y):\n",
    " return np.linalg.inv(X.T.dot(X)).dot(X.T).dot(y)\n",
    "print(least_squares(X,y))\n",
    "\n",
    "# The least squares method is computationally efficient for small datasets but becomes infeasible for large datasets\n",
    "# due to the matrix inversion requirement. It also does not handle multicollinearity well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe97316",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Polynomial regression models the relationship between the independent variable XXX and the dependent variable\n",
    "# yyy as an nnn-th degree polynomial.\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "def polynomial_regression(X, y, degree):\n",
    "    poly = PolynomialFeatures(degree)\n",
    "    X_poly = poly.fit_transform(X)\n",
    "    theta = np.linalg.inv(X_poly.T.dot(X_poly)).dot(X_poly.T).dot(y)\n",
    "    return theta\n",
    "print(polynomial_regression(X,y,3))\n",
    "\n",
    "# Polynomial regression can model more complex relationships but may suffer from overfitting, especially when the\n",
    "# degree of the polynomial is high."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc815f1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0.          -0.         367.70385976   6.29885756   0.\n",
      "   0.          -0.           0.         307.6054181    0.        ]\n"
     ]
    }
   ],
   "source": [
    "# Least Absolute Shrinkage and Selection Operator\n",
    "# Lasso Regression is a regularization technique used to prevent overfitting. It improves linear regression by adding a penalty term to the standard regression equation.\n",
    "# It works by minimizing the sum of squared differences between the observed and predicted values by fitting a line to the data.\n",
    "from sklearn.linear_model import Lasso\n",
    "def lasso_regression(X, y, alpha=1.0):\n",
    "    lasso = Lasso(alpha=alpha)\n",
    "    lasso.fit(X, y)\n",
    "    return lasso.coef_\n",
    "print(lasso_regression(X,y))\n",
    "\n",
    "# LASSO is useful for feature selection and can improve model interpretability by reducing the number of features.\n",
    "# However, it may lead to bias in the coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c49546d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  29.46611189  -83.15427636  306.35268015  201.62773437    5.90961437\n",
      "  -29.51549508 -152.04028006  117.3117316   262.94429001  111.87895644]\n"
     ]
    }
   ],
   "source": [
    "# Ridge regression, also known as  L2 regularization, is a technique used in linear regression to address the problem of multicollinearity\n",
    "# among predictor variables. Multicollinearity occurs when independent variables in a regression model are highly correlated, \n",
    "# which can lead to unreliable and unstable estimates of regression coefficients.\n",
    "# Ridge regression is a variation of linear regression, specifically designed to address multicollinearity in the dataset.\n",
    "from sklearn.linear_model import Ridge\n",
    "def ridge_regression(X, y, alpha=1.0):\n",
    "    ridge = Ridge(alpha=alpha)\n",
    "    ridge.fit(X, y)\n",
    "    return ridge.coef_\n",
    "print(ridge_regression(X,y))\n",
    "# Ridge regression reduces overfitting by penalizing large coefficients. It is particularly effective in the presence of\n",
    "# multicollinearity but does not perform feature selection like LASSO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da8c630f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Comparison of Methods\n",
    "\n",
    "# | **Method**               | **Advantages**                                                               | **Disadvantages**                                                        | **Use Cases**                                             |\n",
    "# |----------------------------|----------------------------------------------------------------------------|---------------------------------------------------------------------------|------------------------------------------------------------|\n",
    "# | **Gradient Descent**       | Efficient for large datasets, no need for matrix inversion.                | Requires tuning of learning rate, may converge slowly.                   | Large datasets, online learning.                          |\n",
    "# | **Least Squares**          | Simple, direct computation.                                                | Infeasible for large datasets, sensitive to multicollinearity.           | Small datasets, baseline model.                           |\n",
    "# | **Polynomial Regression**  | Captures non-linear relationships.                                         | Prone to overfitting, complex model interpretation.                      | When relationship between variables is non-linear.        |\n",
    "# | **LASSO Regression**       | Performs feature selection, interpretable model.                          | May introduce bias, depends heavily on regularization term.              | Sparse models, feature selection.                         |\n",
    "# | **Ridge Regression**       | Reduces overfitting, handles multicollinearity well.                       | Does not perform feature selection.                                      | When multicollinearity is a concern.                      |\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
