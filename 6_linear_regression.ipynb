{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd5c2fbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(442, 10)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "data = load_diabetes()\n",
    "X = data.data\n",
    "y = data.target\n",
    "X.shape\n",
    "\n",
    "# Linear regression is a statistical method used to model the relationship between a dependent variable (often\n",
    "# denoted as yyy) and one or more independent variables (denoted as XXX). The goal is to find the linear\n",
    "# relationship that best predicts yyy from XXX."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f42f427",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  6.50828076   1.28806748  20.8265647   15.61326631   7.24508406\n",
      "   5.85554099 -13.90078374  15.01590614  19.99313426  13.39901343]\n"
     ]
    }
   ],
   "source": [
    "# Gradient descent\n",
    "# Gradient descent is an iterative optimization algorithm used to minimize the cost function in linear regression. The\n",
    "# goal is to find the coefficients (weights) of the linear model that minimize the difference between the predicted\n",
    "# and actual values of the dependent variable.\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "def gradient_descent(X, y, alpha=0.01, iterations=1000):\n",
    "    m, n = X.shape\n",
    "    theta = np.zeros(n)\n",
    "    cost_history = []\n",
    "    for _ in range(iterations):\n",
    "        predictions = X.dot(theta)\n",
    "        errors = predictions - y\n",
    "        gradient = (1/m) * X.T.dot(errors)\n",
    "        theta -= alpha * gradient\n",
    "        cost = (1/(2*m)) * np.sum(errors**2)\n",
    "        cost_history.append(cost)\n",
    "    return theta, cost_history\n",
    "theta,cost_history=gradient_descent(X,y)\n",
    "print(theta)\n",
    "\n",
    "\n",
    "# Gradient descent can efficiently find the optimal solution but may converge to a local minimum if the cost\n",
    "# function is non-convex. The learning rate α\\alphaα plays a crucial role in the convergence speed and stability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa0a1cd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ -10.0098663  -239.81564367  519.84592005  324.3846455  -792.17563855\n",
      "  476.73902101  101.04326794  177.06323767  751.27369956   67.62669218]\n"
     ]
    }
   ],
   "source": [
    "# LEAST SQUARES LINEAR REGRESSION\n",
    "\n",
    "# Least squares is a traditional approach to finding the coefficients that minimize the sum of squared differences\n",
    "# between observed and predicted values.\n",
    "\n",
    "def least_squares(X, y):\n",
    " return np.linalg.inv(X.T.dot(X)).dot(X.T).dot(y)\n",
    "print(least_squares(X,y))\n",
    "\n",
    "# The least squares method is computationally efficient for small datasets but becomes infeasible for large datasets\n",
    "# due to the matrix inversion requirement. It also does not handle multicollinearity well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5fe97316",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.16468886e+15 -7.27762660e+12  5.78333416e+15  4.02478758e+12\n",
      "  9.15821182e+12 -4.17396778e+12  4.03229959e+11 -4.23439525e+12\n",
      "  2.81824099e+12 -1.72544286e+12  5.65878986e+12  1.79151459e+03\n",
      " -1.94240520e+13 -3.35423655e+03 -3.99890352e+03  7.99579451e+06\n",
      " -7.02685506e+06 -2.99180951e+06 -1.95202294e+04 -2.62773687e+06\n",
      "  1.11798650e+04 -9.56777103e+17  1.07421946e+13  2.44433516e+13\n",
      " -1.11403473e+13  1.07621504e+12 -1.13016440e+13  7.52191076e+12\n",
      " -4.60522648e+12  1.51033620e+13 -9.12682565e+03  4.05804403e+03\n",
      "  1.81232462e+07 -1.59365415e+07 -6.76426796e+06  2.46433453e+04\n",
      " -5.96033278e+06 -2.50304127e+01 -5.09757469e+03 -4.34209355e+07\n",
      "  3.81661217e+07  1.62299064e+07 -6.39353925e+03  1.42816071e+07\n",
      " -2.02959687e+03  7.20402003e+07 -1.30507252e+08 -2.73074166e+07\n",
      "  1.82477150e+07 -3.29293926e+07 -1.22752484e+07  5.90505947e+07\n",
      "  2.54329098e+07 -1.60558834e+07  3.02233005e+07  1.07888277e+07\n",
      "  1.64010891e+05 -6.77012119e+06  3.56802734e+06  4.60133271e+06\n",
      "  3.88458293e+04 -6.03050012e+06  7.53383819e+03  3.62027434e+06\n",
      "  4.04136200e+06  7.68386827e+01 -8.31291834e+04  7.51764157e+03\n",
      "  2.10581469e+04 -1.48258545e+04 -9.09688642e+05  9.76849315e+05\n",
      "  2.29049142e+05 -1.48825200e+05  1.33171633e+05  1.24904676e+05\n",
      "  3.21671097e+15  5.46577242e+03 -5.58759329e+04  1.85731495e+05\n",
      " -4.26445887e+04 -1.93486677e+05 -1.06431099e+05 -2.22221211e+05\n",
      "  3.42691142e+04  5.44029902e+04 -1.30504263e+05 -9.36915426e+05\n",
      "  8.08451732e+05  1.93597295e+05 -9.42274214e+04  3.62325579e+05\n",
      " -1.23980839e+04  1.08816232e+03  1.22492942e+06 -1.13369900e+06\n",
      " -3.50045680e+05 -2.84929924e+04 -2.00496586e+05  8.44917285e+04\n",
      "  2.07268511e+07 -3.31119859e+07 -1.71741107e+07 -2.74470439e+06\n",
      " -4.30258177e+07  8.18763108e+05  1.32227953e+07  1.40391362e+07\n",
      "  2.42600550e+06  3.67053014e+07 -8.35275770e+05  3.39621786e+06\n",
      "  7.54227068e+05  1.65756284e+07 -3.01405721e+05 -2.97863200e+05\n",
      "  1.17156944e+06  5.47681162e+04 -1.74810075e+06 -2.24801376e+05\n",
      " -1.30134146e+04 -2.54583205e+15 -1.77895607e+15 -4.04792971e+15\n",
      "  1.84489373e+15 -1.78227632e+14  1.87160276e+15 -1.24566248e+15\n",
      "  7.62645775e+14 -2.50118514e+15 -4.35357572e+04  5.73589444e+04\n",
      " -1.99519936e+06  1.51524241e+06  1.23071477e+06  6.40569859e+05\n",
      "  5.17559663e+05  8.17261157e+04 -2.77246442e+04 -8.14743129e+05\n",
      "  5.23381689e+05  3.96628626e+05  1.11679660e+05  3.30821105e+05\n",
      "  1.97339223e+04  7.64493392e+06 -1.42167650e+07 -6.56365724e+06\n",
      " -1.36671761e+06 -4.23548997e+07  1.41607744e+06  6.51552964e+06\n",
      "  5.56454317e+06  9.33683293e+05  3.76322740e+07 -1.12708037e+06\n",
      "  1.86321528e+06  1.76682500e+06  1.59292349e+07 -6.00286703e+05\n",
      "  5.46702824e+05  1.25014781e+05 -9.60967559e+04 -5.46971120e+06\n",
      " -4.83712793e+05 -4.93015307e+04  3.77476615e+04 -7.82690855e+04\n",
      " -1.40163601e+06  1.23198863e+06  3.80764900e+05 -6.54931925e+04\n",
      "  2.58920043e+05  1.16365872e+05  8.65211881e+04 -1.45944993e+06\n",
      "  1.41063636e+06  3.97249257e+05 -1.13539908e+05  4.02988544e+05\n",
      " -1.59598377e+05  2.81249631e+07 -5.38337271e+07 -2.13198766e+07\n",
      "  4.13605212e+06 -8.24475403e+07  6.49339319e+05  2.54831460e+07\n",
      "  2.07399287e+07 -3.36225246e+06  7.42003767e+07 -5.42927265e+05\n",
      "  4.08173567e+06 -1.72377815e+06  3.06401800e+07 -1.69593266e+05\n",
      " -3.02069069e+05 -1.67420290e+06 -5.85595846e+04 -7.51678911e+06\n",
      " -5.13721807e+04 -8.78005556e+04  2.03269129e+04 -1.09656716e+06\n",
      "  1.00469912e+06  4.56863267e+05  2.17770866e+03  3.41357492e+05\n",
      " -3.82335046e+04 -6.15163863e+07  1.09114120e+08  4.78804565e+07\n",
      " -3.43043125e+05  1.99247541e+08  7.12421608e+05 -4.82957770e+07\n",
      " -4.28129856e+07 -1.39396674e+05 -1.75391348e+08 -6.33916591e+05\n",
      " -9.16717017e+06  8.22316043e+05 -7.52519075e+07 -3.53052512e+05\n",
      "  6.27208112e+05 -8.73324637e+04 -2.50711375e+05  1.68999123e+07\n",
      "  5.63716464e+04 -1.38756321e+04  9.94337127e+07 -2.84769652e+08\n",
      " -6.31395331e+07  5.02034076e+07 -3.40578507e+08 -1.98760746e+07\n",
      "  2.64677811e+08  1.32262940e+08 -7.89148218e+07  6.33072487e+08\n",
      "  3.65684600e+07  6.41580913e+06 -3.76883247e+07  1.25303511e+08\n",
      "  1.35835842e+07 -5.77355743e+05 -1.04415921e+08 -3.92879588e+06\n",
      "  1.66633919e+07  5.84379161e+07 -1.28887423e+06 -8.00364219e+07\n",
      " -6.56659150e+07  3.02730847e+07 -2.91221450e+08 -1.69749474e+07\n",
      " -1.03811754e+07  2.99136790e+07 -1.24829851e+08 -1.22661652e+07\n",
      "  1.12618749e+06  8.76432749e+07  3.72996520e+06 -1.64791531e+07\n",
      " -5.20060813e+07  1.27414203e+06  1.27654436e+06  7.01146005e+06\n",
      "  5.12824486e+05 -2.22011898e+06 -5.41836525e+05  3.96213735e+07\n",
      "  1.86899085e+06 -1.54646637e+07 -2.13055083e+07  3.36368561e+05\n",
      " -7.55506697e+05  1.10821748e+06  1.79611615e+05 -3.77138221e+06\n",
      "  1.79710675e+06 -2.20384005e+05  4.85787454e+06  4.20276261e+06\n",
      "  4.09996977e+05  1.34634278e+04]\n"
     ]
    }
   ],
   "source": [
    "# Polynomial regression models the relationship between the independent variable XXX and the dependent variable\n",
    "# yyy as an nnn-th degree polynomial.\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "def polynomial_regression(X, y, degree):\n",
    "    poly = PolynomialFeatures(degree)\n",
    "    X_poly = poly.fit_transform(X)\n",
    "    theta = np.linalg.inv(X_poly.T.dot(X_poly)).dot(X_poly.T).dot(y)\n",
    "    return theta\n",
    "print(polynomial_regression(X,y,3))\n",
    "\n",
    "# Polynomial regression can model more complex relationships but may suffer from overfitting, especially when the\n",
    "# degree of the polynomial is high."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc815f1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0.          -0.         367.70385976   6.29885756   0.\n",
      "   0.          -0.           0.         307.6054181    0.        ]\n"
     ]
    }
   ],
   "source": [
    "# Least Absolute Shrinkage and Selection Operator\n",
    "# Lasso Regression is a regularization technique used to prevent overfitting. It improves linear regression by adding a penalty term to the standard regression equation.\n",
    "# It works by minimizing the sum of squared differences between the observed and predicted values by fitting a line to the data.\n",
    "from sklearn.linear_model import Lasso\n",
    "def lasso_regression(X, y, alpha=1.0):\n",
    "    lasso = Lasso(alpha=alpha)\n",
    "    lasso.fit(X, y)\n",
    "    return lasso.coef_\n",
    "print(lasso_regression(X,y))\n",
    "\n",
    "# LASSO is useful for feature selection and can improve model interpretability by reducing the number of features.\n",
    "# However, it may lead to bias in the coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c49546d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  29.46611189  -83.15427636  306.35268015  201.62773437    5.90961437\n",
      "  -29.51549508 -152.04028006  117.3117316   262.94429001  111.87895644]\n"
     ]
    }
   ],
   "source": [
    "# Ridge regression, also known as  L2 regularization, is a technique used in linear regression to address the problem of multicollinearity\n",
    "# among predictor variables. Multicollinearity occurs when independent variables in a regression model are highly correlated, \n",
    "# which can lead to unreliable and unstable estimates of regression coefficients.\n",
    "# Ridge regression is a variation of linear regression, specifically designed to address multicollinearity in the dataset.\n",
    "from sklearn.linear_model import Ridge\n",
    "def ridge_regression(X, y, alpha=1.0):\n",
    "    ridge = Ridge(alpha=alpha)\n",
    "    ridge.fit(X, y)\n",
    "    return ridge.coef_\n",
    "print(ridge_regression(X,y))\n",
    "# Ridge regression reduces overfitting by penalizing large coefficients. It is particularly effective in the presence of\n",
    "# multicollinearity but does not perform feature selection like LASSO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "da8c630f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Comparison of Methods\n",
    "\n",
    "# | **Method**               | **Advantages**                                                               | **Disadvantages**                                                        | **Use Cases**                                             |\n",
    "# |----------------------------|----------------------------------------------------------------------------|---------------------------------------------------------------------------|------------------------------------------------------------|\n",
    "# | **Gradient Descent**       | Efficient for large datasets, no need for matrix inversion.                | Requires tuning of learning rate, may converge slowly.                   | Large datasets, online learning.                          |\n",
    "# | **Least Squares**          | Simple, direct computation.                                                | Infeasible for large datasets, sensitive to multicollinearity.           | Small datasets, baseline model.                           |\n",
    "# | **Polynomial Regression**  | Captures non-linear relationships.                                         | Prone to overfitting, complex model interpretation.                      | When relationship between variables is non-linear.        |\n",
    "# | **LASSO Regression**       | Performs feature selection, interpretable model.                          | May introduce bias, depends heavily on regularization term.              | Sparse models, feature selection.                         |\n",
    "# | **Ridge Regression**       | Reduces overfitting, handles multicollinearity well.                       | Does not perform feature selection.                                      | When multicollinearity is a concern.                      |\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
