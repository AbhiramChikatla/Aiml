{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a063096e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the decision tree classifier compare its performance with ensemble techniques like random forest, bagging,\n",
    "# boosting and stacking Demonstrate it with different decision trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fad1414",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The task involves building a Decision Tree classifier and comparing its performance with various ensemble\n",
    "# techniques, such as Random Forest, Bagging, Boosting, and Stacking. The Decision Tree classifier is a simple yet\n",
    "# powerful machine learning algorithm used for classification tasks. Ensemble methods, on the other hand, combine\n",
    "# multiple models to improve predictive performance, reduce overfitting, and increase robustness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e76e54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree - Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1 Score:1.0000\n",
      "Random Forest - Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1 Score:1.0000\n",
      "Bagging - Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1 Score:1.0000\n",
      "Boosting - Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1 Score:1.0000\n",
      "Stacking - Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1 Score:1.0000\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, BaggingClassifier, AdaBoostClassifier, StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "# Load dataset\n",
    "data = load_iris()\n",
    "X, y = data.data, data.target\n",
    "# Split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "# Initialize models\n",
    "dt_model = DecisionTreeClassifier(random_state=42)\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "bagging_model = BaggingClassifier(estimator=DecisionTreeClassifier(), random_state=42)\n",
    "boosting_model = AdaBoostClassifier(estimator=DecisionTreeClassifier(), random_state=42)\n",
    "# Stacking model\n",
    "estimators = [\n",
    " ('dt', DecisionTreeClassifier(random_state=42)),\n",
    " ('svc', SVC(probability=True, random_state=42))\n",
    "]\n",
    "stacking_model = StackingClassifier(estimators=estimators, final_estimator=LogisticRegression())\n",
    "# Train models\n",
    "dt_model.fit(X_train, y_train)\n",
    "rf_model.fit(X_train, y_train)\n",
    "bagging_model.fit(X_train, y_train)\n",
    "boosting_model.fit(X_train, y_train)\n",
    "stacking_model.fit(X_train, y_train)\n",
    "# Predictions\n",
    "dt_pred = dt_model.predict(X_test)\n",
    "rf_pred = rf_model.predict(X_test)\n",
    "bagging_pred = bagging_model.predict(X_test)\n",
    "boosting_pred = boosting_model.predict(X_test)\n",
    "stacking_pred = stacking_model.predict(X_test)\n",
    "# Evaluation\n",
    "def evaluate_model(y_test, y_pred, model_name):\n",
    " accuracy = accuracy_score(y_test, y_pred)\n",
    " precision = precision_score(y_test, y_pred, average='macro')\n",
    " recall = recall_score(y_test, y_pred, average='macro')\n",
    " f1 = f1_score(y_test, y_pred, average='macro')\n",
    " print(f\"{model_name} - Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score:{f1:.4f}\")\n",
    "evaluate_model(y_test, dt_pred, \"Decision Tree\")\n",
    "evaluate_model(y_test, rf_pred, \"Random Forest\")\n",
    "evaluate_model(y_test, bagging_pred, \"Bagging\")\n",
    "evaluate_model(y_test, boosting_pred, \"Boosting\")\n",
    "evaluate_model(y_test, stacking_pred, \"Stacking\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "875c6107",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expected Results:\n",
    "# ÔÇ∑ Decision Tree: Likely to have decent performance but may overfit, especially with complex datasets.\n",
    "# ÔÇ∑ Random Forest: Expected to perform better due to reduced overfitting and better generalization by\n",
    "# averaging multiple trees.\n",
    "# ÔÇ∑ Bagging: Should provide improvement over a single Decision Tree by reducing variance.\n",
    "# ÔÇ∑ Boosting: Likely to outperform others, as it focuses on hard-to-classify instances and reduces both bias and\n",
    "# variance.\n",
    "# ÔÇ∑ Stacking: Expected to perform well as it combines multiple models' strengths using a meta-learner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a6aa664d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trained models and their evaluation metrics (accuracy, precision, recall, F1 score) for comparison.\n",
    "# Sample Output:\n",
    "# Decision Tree - Accuracy: 0.9778, Precision: 0.9778, Recall: 0.9780, F1 Score: 0.9780\n",
    "# Random Forest - Accuracy: 0.9778, Precision: 0.9778, Recall: 0.9780, F1 Score: 0.9780\n",
    "# Bagging - Accuracy: 0.9778, Precision: 0.9778, Recall: 0.9780, F1 Score: 0.9780\n",
    "# Boosting - Accuracy: 0.9778, Precision: 0.9778, Recall: 0.9780, F1 Score: 0.9780\n",
    "# Stacking - Accuracy: 0.9778, Precision: 0.9778, Recall: 0.9780, F1 Score: 0.9780\n",
    "# The analysis would discuss the relative strengths of each approach and the scenarios in which one might\n",
    "# outperform the others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fd40a9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Of course! Let‚Äôs go through it in a *very simple and clear way* üåü:  \n",
    "# (Imagine you are teaching a class ‚Äî this would be the perfect way to explain!)\n",
    "\n",
    "# ---\n",
    "\n",
    "# # üå≥ 1. Bagging (Bootstrap Aggregating)\n",
    "\n",
    "# - *Idea*:  \n",
    "#   Train *many models independently* (like many Decision Trees) on *different random parts* of the dataset, and *combine* their predictions.\n",
    "\n",
    "# - *How*:  \n",
    "#   Randomly pick data with replacement ‚Üí train many models ‚Üí average (for regression) or vote (for classification).\n",
    "\n",
    "# - *Goal*:  \n",
    "#   *Reduce variance* (make the model more stable).\n",
    "\n",
    "# - *Example*:  \n",
    "#   *Random Forest* uses bagging with Decision Trees!\n",
    "\n",
    "# ---\n",
    "# # üéØ 2. Random Forest\n",
    "\n",
    "# - *Idea*:  \n",
    "#   Random Forest = *Bagging + extra randomness* in tree building.\n",
    "\n",
    "# - *How*:  \n",
    "#   - Randomly sample the *data rows* (like Bagging).  \n",
    "#   - Also randomly sample the *features/columns* at each tree split!\n",
    "\n",
    "# - *Goal*:  \n",
    "#   *More diverse trees* ‚Üí better performance ‚Üí less overfitting.\n",
    "\n",
    "# - *Example*:  \n",
    "#   If you have 10 features, Random Forest might pick only 3 random features when splitting each node!\n",
    "\n",
    "# ---\n",
    "# # üöÄ 3. Boosting\n",
    "\n",
    "# - *Idea*:  \n",
    "#   Train models *one after another, and each new model **fixes the mistakes* of the previous ones.\n",
    "\n",
    "# - *How*:  \n",
    "#   - First model makes predictions.  \n",
    "#   - Second model focuses more on the points where the first model failed.  \n",
    "#   - Third model fixes the second, and so on.\n",
    "#   - Combine all models' predictions at the end.\n",
    "\n",
    "# - *Goal*:  \n",
    "#   *Reduce bias* (make the model smarter).\n",
    "\n",
    "# - *Popular algorithms*:  \n",
    "#   - AdaBoost  \n",
    "#   - Gradient Boosting  \n",
    "#   - XGBoost (super popular)  \n",
    "#   - LightGBM  \n",
    "\n",
    "# ---\n",
    "# # üèó 4. Stacking\n",
    "\n",
    "# - *Idea*:  \n",
    "#   Train *different kinds of models* (like a Decision Tree, a Logistic Regression, and an SVM) and then *combine their outputs* using a *new model* (meta-model).\n",
    "\n",
    "# - *How*:  \n",
    "#   - Train multiple *base models*.\n",
    "#   - Use their predictions as inputs to a *final model* (called a blender or meta-learner).\n",
    "#   - The final model learns how to best combine them.\n",
    "\n",
    "# - *Goal*:  \n",
    "#   *Leverage the strengths* of different models to make better predictions.\n",
    "\n",
    "# - *Example*:  \n",
    "#   Train a Random Forest + SVM + KNN ‚Üí combine their outputs using Logistic Regression.\n",
    "\n",
    "# ---\n",
    "\n",
    "# # üìä Summary Table\n",
    "\n",
    "# | Method     | Strategy                 | Purpose                |\n",
    "# |------------|---------------------------|-------------------------|\n",
    "# | Bagging    | Many models in parallel    | Reduce variance         |\n",
    "# | Random Forest | Bagging + random features | Better than simple Bagging |\n",
    "# | Boosting   | Models in sequence         | Reduce bias             |\n",
    "# | Stacking   | Mix different models       | Combine strengths       |\n",
    "\n",
    "# ---\n",
    "\n",
    "# Would you also like me to draw a simple *flow diagram* showing Bagging vs Boosting vs Stacking visually? üé®‚ú® (very easy to understand with a picture!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eafd0a4c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
